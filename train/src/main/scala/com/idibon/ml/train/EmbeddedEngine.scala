package com.idibon.ml.train

import com.idibon.ml.alloy.IntentAlloy
import com.idibon.ml.feature.{DocumentExtractor, FeaturePipeline, FeaturePipelineBuilder}
import com.idibon.ml.feature.indexer.IndexTransformer
import com.idibon.ml.feature.tokenizer.TokenTransformer
import com.idibon.ml.predict.Engine

import org.apache.spark.mllib.regression.LabeledPoint
import org.json4s._
import org.json4s.JsonDSL._
import org.json4s.native.JsonMethods.{compact, parse, render}
import scala.collection.mutable.ListBuffer
import scala.io.Source

class EmbeddedEngine extends Engine {

  def start() = {
    // Train a model
    val pipelineConfig : JObject = (
      "transforms" -> List(
        ("name" -> "convertToIndex") ~ ("class" -> "com.idibon.ml.feature.indexer.IndexTransformer"),
        ("name" -> "convertToTokens") ~ ("class" -> "com.idibon.ml.feature.tokenizer.TokenTransformer"),
        ("name" -> "contentExtractor") ~ ("class" -> "com.idibon.ml.feature.DocumentExtractor"))) ~
      ("pipeline" -> List(
        ("name" -> "$output") ~ ("inputs" -> List("convertToIndex")),
        ("name" -> "convertToIndex") ~ ("inputs" -> List("convertToTokens")),
        ("name" -> "convertToTokens") ~ ("inputs" -> List("contentExtractor")),
        ("name" -> "contentExtractor") ~ ("inputs" -> List("$document"))))

    val alloy = new IntentAlloy()

    val pipeline = (FeaturePipelineBuilder.named("IntentPipeline")
      += (FeaturePipelineBuilder.entry("convertToIndex", new IndexTransformer, "convertToTokens"))
      += (FeaturePipelineBuilder.entry("convertToTokens", new TokenTransformer, "contentExtractor"))
      += (FeaturePipelineBuilder.entry("contentExtractor", new DocumentExtractor, "$document"))
      := ("convertToIndex"))

    val doc : JObject = ( "content" -> "colorless green ideas sleep furiously" )

    val results = pipeline(doc)
    println(s"results: $results")

    // Load the data generated by idibin/bin/open_source_integration/export_training_to_idiml.rb
    val filename = "/tmp/idiml.txt"
    // Using a loop here because operations can't occur in parallel since the pipeline accumulates index values over time.
    val intentLabeledPoints = new ListBuffer[LabeledPoint]()
    for (line <- Source.fromFile(filename).getLines()) {
      val json = parse(line)

      // Create labeled points
      if (json \ "label" == JString("Intent to Buy")) {
        val featureVector = pipeline(json.asInstanceOf[JObject]).head
        intentLabeledPoints += LabeledPoint(1.0, featureVector)
      }
    }

    println(s"intentLabeledPoints: $intentLabeledPoints")

    // Save the pipeline definition
    val test = pipeline.save(alloy.writer.within("IntentPipeline"))
    val alloyMetaJson: JObject = ("IntentPipeline" -> test)
    val writer = alloy.writer.within("IntentPipeline").resource("config.json")
    writer.writeInt(compact(render(alloyMetaJson)).length)
    writer.writeChars(compact(render(alloyMetaJson)))

    // Load the pipeline definition again
    val reader = alloy.reader.within("IntentPipeline").resource("config.json")
    val size = reader.readInt()
    val charBuffer = new ListBuffer[Char]
    1 to size foreach { _ =>
      charBuffer += reader.readChar()
    }
    val newPipelineConfig: JObject = (parse(charBuffer.mkString) \ "IntentPipeline").asInstanceOf[JObject]
    val newPipeline2 = (new FeaturePipeline).load(alloy.reader().within("IntentPipeline"), Some(newPipelineConfig))

    // This should be the same as $results
    val results2 = newPipeline2(doc)
    println(s"results2: $results2")

    // This should prove to me that my training data is intact (index values are being referenced)
    val doc2 : JObject = ( "content" -> "some kind of crazy document with made-up words like fdsafafa and feafdasfdas")
    val results3 = newPipeline2(doc2)
    println(s"results3: $results3")

  }
}

