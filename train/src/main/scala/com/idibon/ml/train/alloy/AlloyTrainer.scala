package com.idibon.ml.train.alloy

import java.util

import com.idibon.ml.alloy.{ScalaJarAlloy, Alloy}
import com.idibon.ml.common.Engine
import com.idibon.ml.feature.bagofwords.{CaseTransform, BagOfWordsTransformer}
import com.idibon.ml.feature.indexer.IndexTransformer
import com.idibon.ml.feature.language.LanguageDetector
import com.idibon.ml.feature.ngram.NgramTransformer
import com.idibon.ml.feature.tokenizer.{TokenTransformer, Tag}
import com.idibon.ml.feature.{ContentExtractor, FeaturePipelineBuilder, FeaturePipeline}
import com.idibon.ml.predict.PredictModel
import com.idibon.ml.predict.ensemble.EnsembleModel
import com.idibon.ml.predict.ml.{MLModel}
import com.idibon.ml.predict.rules.DocumentRules
import com.idibon.ml.train.{SparkDataGenerator}
import com.idibon.ml.train.furnace.Furnace
import com.typesafe.scalalogging.StrictLogging
import org.apache.spark.sql.DataFrame
import org.json4s.JObject

import scala.collection.mutable
import scala.util.{Failure, Try}

/**
  * This is the trait that an alloy trainer implements.
  *
  * It contains anything that can be used by all and is fairly static, like rules generation.
  *
  * @author "Stefan Krawczyk <stefan@idibon.com>"
  */
trait AlloyTrainer {
  /** Trains a model and generates an Alloy from it
    *
    * Callers must provide a callback function which returns a traversable
    * list of documents; this function will be called multiple times, and
    * each invocation of the function must return an instance that will
    * traverse over the exact set of documents traversed by previous instances.
    *
    * Traversed documents should match the format generated by
    * idibin.git:/idibin/bin/open_source_integration/export_training_to_idiml.rb
    *
    *   { "content": "Who drives a chevy maliby Would you recommend it?
    *     "metadata": { "iso_639_1": "en" },
    *     "annotations: [{ "label": { "name": "Intent" }, "isPositive": true }]}
    *
    * @param docs - a callback function returning a traversable sequence
    *   of JSON training documents, such as those generated by export_training_to_idiml.rb
    * @param rules a callback function returning a traversable sequence
    *   of JSON Rules, one rule per label per line, such as those generated by export_training_to_idiml.rb.
    * @param config training configuration parameters. Optional.
    * @return an Alloy with the trained model
    */
  def trainAlloy(docs: () => TraversableOnce[JObject],
                 rules: () => TraversableOnce[JObject],
                 config: Option[JObject]): Try[Alloy]

  /**
    * Creates a map of label to rules from some JSON data.
    *
    * It expects something like {"label": label, "rule": "REGEX or TEXT", "weight": WEIGHT_VALUE}.
    *
    * @param rules a callback function returning a traversable sequence
    *   of JSON Rules, one rule per label per line, such as those generated by export_training_to_idiml.rb.
    * @return a map of label -> list of rules.
    */
  def rulesGenerator(rules: () => TraversableOnce[JObject]): Map[String, List[(String, Float)]] = {
    implicit val formats = org.json4s.DefaultFormats

    rules().map(x => {
      val label = (x \ "label").extract[String]
      val expression = (x \ "expression").extract[String]
      val weight = (x \ "weight").extract[Float]
      (label, expression, weight)
    }).foldRight(new mutable.HashMap[String, List[(String, Float)]]()){
      case ((label, expression, weight), map) => {
        val list = map.getOrElse(label, List[(String, Float)]())
        map.put(label, list :+ (expression, weight))
        map
      }
    }.toMap
  }

  /**
    * Default implemenation of taking MLModels and creating Predict models that have rules with them.
    *
    * @param models
    * @param rules
    * @return
    */
  def mergeRulesWithModels(models: Map[String, MLModel],
                           rules: Map[String, List[(String, Float)]]): Map[String, PredictModel] = {
    models.map({case (label, mlmodel) => {
      // Rule
      val ruleModel = new DocumentRules(label, rules.getOrElse(label, List()))
      // Ensemble
      val ensembleModel = new EnsembleModel(label, List[PredictModel](mlmodel, ruleModel))
      (label, ensembleModel)
    }})
  }
}

/**
  * Base class for trainers that follow a fairly orthodox approach to training.
  *
  * @param engine
  * @param dataGen
  * @param furnace
  */
abstract class BaseTrainer[T](engine: Engine,
                              dataGen: SparkDataGenerator[T],
                              furnace: Furnace[T]) extends AlloyTrainer {

  /** Trains a model and generates an Alloy from it
    *
    * Callers must provide a callback function which returns a traversable
    * list of documents; this function will be called multiple times, and
    * each invocation of the function must return an instance that will
    * traverse over the exact set of documents traversed by previous instances.
    *
    * Traversed documents should match the format generated by
    * idibin.git:/idibin/bin/open_source_integration/export_training_to_idiml.rb
    *
    * { "content": "Who drives a chevy maliby Would you recommend it?
    * "metadata": { "iso_639_1": "en" },
    * "annotations: [{ "label": { "name": "Intent" }, "isPositive": true }]}
    *
    * @param docs     - a callback function returning a traversable sequence
    *                 of JSON training documents, such as those generated by export_training_to_idiml.rb
    * @param rules    a callback function returning a traversable sequence
    *                 of JSON Rules, one rule per label per line, such as those generated by export_training_to_idiml.rb.
    * @param config   training configuration parameters. Optional.
    * @return an Alloy with the trained model
    */
  override def trainAlloy(docs: () => TraversableOnce[JObject],
                          rules: () => TraversableOnce[JObject],
                          config: Option[JObject]): Try[Alloy] = {
    implicit val formats = org.json4s.DefaultFormats
    // Create uuidsByLabel TODO: Figure out how this should be passed in.
    val uuidsByLabel = Map[String, String]()
    // Parse Rules
    val parsedRules = rulesGenerator(rules)
    /* delegate to trainer:
      - creating pipelines,
      - training MLModels
     */
    val mlModels = {
      this.melt(docs, dataGen, config.map(c => (c \ "pipelineConfig").extract[JObject]))
    }
    mlModels
      // create PredictModels with Rules
      .map(this.mergeRulesWithModels(_, parsedRules))
      // create Alloy
      .map(new ScalaJarAlloy(_, uuidsByLabel))
  }

  /**
    * This is the method where each alloy trainer does its magic and creates the MLModel(s) required.
    *
    * @param rawData
    * @param dataGen
    * @param pipelineConfig
    * @return
    */
  def melt(rawData: () => TraversableOnce[JObject],
           dataGen: SparkDataGenerator[T],
           pipelineConfig: Option[JObject]): Try[Map[String, MLModel]]

}

/**
  * Trains K models using a global feature pipeline.
  *
  * @param engine
  * @param dataGen
  * @param furnace
  */
class KClass1FP(engine: Engine,
                dataGen: SparkDataGenerator[DataFrame],
                furnace: Furnace[DataFrame])
  extends BaseTrainer[DataFrame](engine, dataGen, furnace) with StrictLogging {

  /**
    * Implements the overall algorithm for putting together the pieces required for an alloy.
    *
    * @param rawData
    * @param dataGen
    * @param pipelineConfig
    * @return
    */
  override def melt(rawData: () => TraversableOnce[JObject],
                    dataGen: SparkDataGenerator[DataFrame],
                    pipelineConfig: Option[JObject]): Try[Map[String, MLModel]] = {
    // create one feature pipeline
    val rawPipeline = pipelineConfig match {
      case Some(config) => createFeaturePipeline(config)
      case _ => return Failure(new IllegalArgumentException("No feature pipeline config passed."))
    }
    // prime the pipeline
    val primedPipeline = rawPipeline.prime(rawData())
    // create featurized data once since we only have one feature pipeline
    val featurizedData = furnace.featurizeData(rawData, dataGen, primedPipeline)
    val featuresUsed = new util.HashSet[Int](100000)
    // delegate to the furnace for producing MLModels for each label
    val models = featurizedData match {
      case Some(featureData) => featureData.par.map {
        case (label, data) => {
          val model = furnace.fit(label, data, primedPipeline)
          (label, model, model.getFeaturesUsed())
        }
      }.toList.map({ // remove parallelism and gather all features used.
        case (label, model, usedFeatures) => {
          // add what was used so we can prune it from the global feature pipeline.
          usedFeatures.foreachActive((index, _) => featuresUsed.add(index))
          (label, model)
        }
      })
      case None => return Failure(new RuntimeException("Failed to create training data."))
    }
    logger.info(s"Fitted models, ${featuresUsed.size()} features used.")
    // function to pass down so that the feature transforms can prune themselves.
    // i.e. if it isn't used, remove it.
    def isNotUsed(featureIndex: Int): Boolean = {
      !featuresUsed.contains(featureIndex)
    }
    // prune unused features from global feature pipeline
    primedPipeline.prune(isNotUsed)
    // return MLModels
    Try(models.toMap)
  }

  /**
    * Creates a single feature pipeline from the passed in configuration.
    *
    * @param config
    * @return
    */
  def createFeaturePipeline(config: JObject): FeaturePipeline = {
    implicit val formats = org.json4s.DefaultFormats
    val ngramSize = (config \ "ngram").extract[Int]
    //TODO: unhardcode this
    (FeaturePipelineBuilder.named("pipeline")
      += FeaturePipelineBuilder.entry("convertToIndex", new IndexTransformer, "ngrams")
      += FeaturePipelineBuilder.entry("ngrams", new NgramTransformer(1, ngramSize), "bagOfWords")
      += FeaturePipelineBuilder.entry("bagOfWords",
      new BagOfWordsTransformer(List(Tag.Word, Tag.Punctuation), CaseTransform.ToLower),
      "convertToTokens", "languageDetector")
      += FeaturePipelineBuilder.entry("convertToTokens", new TokenTransformer, "contentExtractor", "languageDetector")
      += FeaturePipelineBuilder.entry("languageDetector", new LanguageDetector, "$document")
      += FeaturePipelineBuilder.entry("contentExtractor", new ContentExtractor, "$document")
      := "convertToIndex")
  }
}

///**
//  * Trains K models using K feature pipelines - one for each model.
//  *
//  * @param engine
//  * @param rDDGenerator
//  * @param featurePipelines
//  * @param furnace
//  */
//class KClassKFP(engine: Engine,
//                rDDGenerator: RDDGenerator,
//                featurePipelines: Map[String, FeaturePipeline],
//                furnace: Furnace)
//  extends BaseTrainer(engine, rDDGenerator) with StrictLogging {
//
//}
//
//
///**
//  * Trains 1 multi-class model using a single feature pipeline.
//  *
//  * @param engine
//  * @param rDDGenerator
//  * @param featurePipeline
//  * @param furnace
//  */
//class MultiClass1FP(engine: Engine,
//                rDDGenerator: RDDGenerator,
//                featurePipeline: FeaturePipeline,
//                furnace: Furnace)
//  extends BaseTrainer(engine, rDDGenerator) with StrictLogging {
//
//}
